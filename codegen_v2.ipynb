{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saxenaya/miniconda3/envs/robot_commands/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display --no-stderr --no-stdout\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from generate_with_embeddings import GenerateWithEmbeddings\n",
    "from utils import ConstantLengthDataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DSL.txt\", 'r') as f:\n",
    "    prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "class CodeDataset(Dataset): # NOTE: this isn't very space-efficient since it has to load in the entire dataset at once\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.data = []\n",
    "        for d in os.listdir(self.data_dir):\n",
    "            code_dir = os.path.join(self.data_dir, d, \"code.txt\")\n",
    "            prompts_dir = os.path.join(self.data_dir, d, \"prompts.txt\")\n",
    "\n",
    "            with open(code_dir, 'r') as f:\n",
    "                code = f.read()\n",
    "            \n",
    "            with open(prompts_dir, 'r') as f:\n",
    "                prompts = f.read()\n",
    "            \n",
    "            prompts = prompts.split(\"\\n\")\n",
    "\n",
    "            for p in prompts:\n",
    "                self.data.append((code, prompt + p + \"\\n\\t\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.data_dir))\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = tokenizer(self.data[idx][1], padding=\"max_length\", max_length=4)[\"input_ids\"]\n",
    "        labels = tokenizer(self.data[idx][0], padding=\"max_length\", max_length=4)[\"input_ids\"]\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bigcode/santacoder\" # \"facebook/incoder-6B\" # \n",
    "revision = \"dedup-alt\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, revision=revision)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, revision=revision, trust_remote_code=True).to(device)\n",
    "\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def _stop_at_stop_token(decoded_string, stop_tokens):\n",
    "    \"\"\"\n",
    "    Produces the prefix of decoded_string that ends at the first occurrence of\n",
    "    a stop_token.\n",
    "\n",
    "    WARNING: the decoded_string *must not* include the prompt, which may have stop tokens\n",
    "    itself.\n",
    "    \"\"\"\n",
    "    min_stop_index = len(decoded_string)\n",
    "    for stop_token in stop_tokens:\n",
    "        stop_index = decoded_string.find(stop_token)\n",
    "        if stop_index != -1 and stop_index < min_stop_index:\n",
    "            min_stop_index = stop_index\n",
    "    return decoded_string[:min_stop_index]\n",
    "\n",
    "\n",
    "class ConstantLengthDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n",
    "        Args:\n",
    "            tokenizer (Tokenizer): The processor used for proccessing the data.\n",
    "            dataset (dataset.Dataset): Dataset with text files.\n",
    "            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n",
    "            seq_length (int): Length of token sequences to return.\n",
    "            num_of_sequences (int): Number of token sequences to keep in buffer.\n",
    "            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        dataset,\n",
    "        infinite=False,\n",
    "        seq_length=4,\n",
    "        num_of_sequences=4,\n",
    "        chars_per_token=3.6,\n",
    "        content_field=\"content\",\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = (\n",
    "            tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.encode(\n",
    "                \"<|endoftext|>\")\n",
    "        )\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.infinite = infinite\n",
    "        self.current_size = 0\n",
    "        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n",
    "        self.content_field = content_field\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            input_ids_buffer, labels_buffer, buffer_len = [], [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.max_buffer_size:\n",
    "                    break\n",
    "                try:\n",
    "                    input_ids_buffer.append(next(iterator)[\"input_ids\"])\n",
    "                    labels_buffer.append(next(iterator)[\"labels\"])\n",
    "                    buffer_len += len(input_ids_buffer[-1])\n",
    "                except StopIteration:\n",
    "                    if self.infinite:\n",
    "                        iterator = iter(self.dataset)\n",
    "                    else:\n",
    "                        more_examples = False\n",
    "                        break\n",
    "\n",
    "            all_token_ids, all_labels = [], []\n",
    "\n",
    "            for token_ids, labels in zip(input_ids_buffer, labels_buffer):\n",
    "                all_token_ids.extend(token_ids + [self.concat_token_id])\n",
    "                all_labels.extend(labels + [self.concat_token_id])\n",
    "\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i: i + self.seq_length]\n",
    "                labels = all_labels[i: i + self.seq_length]\n",
    "\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    self.current_size += 1\n",
    "\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.LongTensor(input_ids),\n",
    "                        \"labels\": torch.LongTensor(labels)\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import ConstantLengthDataset\n",
    "\n",
    "batch_size = 16\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")\n",
    "\n",
    "train_code_dataset = CodeDataset(\"./train\")\n",
    "train_dataset = ConstantLengthDataset(tokenizer, train_code_dataset)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                         batch_size,\n",
    "                                         collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iterator = iter(train_dataset)\n",
    "res = next(dataset_iterator)\n",
    "\n",
    "input_ids, labels = res['input_ids'], res['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''\n",
      "def go\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go_to(\"\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "learning_rate = 2e-5\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 0/100 [00:02<?, ?it/s]\n",
      "  0%|          | 0/100 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 23.65 GiB total capacity; 18.73 GiB already allocated; 268.12 MiB free; 19.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_dataloader))\n\u001b[1;32m     11\u001b[0m gwe \u001b[39m=\u001b[39m GenerateWithEmbeddings(model,\n\u001b[1;32m     12\u001b[0m                              tokenizer,\n\u001b[1;32m     13\u001b[0m                              \u001b[39mNone\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m                              device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m     17\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m---> 18\u001b[0m loss \u001b[39m=\u001b[39m gwe\u001b[39m.\u001b[39;49mgenerate_step()\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(epoch, step, loss\u001b[39m.\u001b[39mitem(),gwe\u001b[39m.\u001b[39mlast_predictions)\n",
      "File \u001b[0;32m~/robot_commands/generate_with_embeddings.py:129\u001b[0m, in \u001b[0;36mGenerateWithEmbeddings.generate_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mReached max_length tokens to generate.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m cur_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_ids \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    130\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minputs_embeds,\n\u001b[1;32m    131\u001b[0m     attention_mask\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_mask,\n\u001b[1;32m    132\u001b[0m     labels\u001b[39m=\u001b[39;49mcur_labels,\n\u001b[1;32m    133\u001b[0m     output_attentions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    136\u001b[0m next_tokens_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_outputs\u001b[39m.\u001b[39mlogits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1075\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1075\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1076\u001b[0m     input_ids,\n\u001b[1;32m   1077\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1078\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1079\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1080\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1081\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1082\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1083\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1084\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1085\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1086\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1087\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1088\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1089\u001b[0m )\n\u001b[1;32m   1090\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:899\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    889\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    890\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    891\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    900\u001b[0m         hidden_states,\n\u001b[1;32m    901\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    902\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    903\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    904\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    905\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    906\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    907\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    908\u001b[0m     )\n\u001b[1;32m    910\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    911\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:426\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    425\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 426\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    427\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:354\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[39m.\u001b[39mFloatTensor]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m    353\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> 354\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(hidden_states)\n\u001b[1;32m    355\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    356\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/transformers/activations.py:87\u001b[0m, in \u001b[0;36mFastGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39minput\u001b[39;49m \u001b[39m*\u001b[39;49m (\u001b[39m1.0\u001b[39;49m \u001b[39m+\u001b[39;49m torch\u001b[39m.\u001b[39;49mtanh(\u001b[39minput\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m0.7978845608\u001b[39;49m \u001b[39m*\u001b[39;49m (\u001b[39m1.0\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m0.044715\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39minput\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39minput\u001b[39;49m)))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 23.65 GiB total capacity; 18.73 GiB already allocated; 268.12 MiB free; 19.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "train_size_per_epoch = 100\n",
    "step = 1\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0\n",
    "    stats = {\"correct\": 0, \"total\": 0}\n",
    "\n",
    "    for _ in tqdm(range(train_size_per_epoch)):\n",
    "        inputs = next(iter(train_dataloader))\n",
    "        gwe = GenerateWithEmbeddings(model,\n",
    "                                     tokenizer,\n",
    "                                     None, \n",
    "                                     None,\n",
    "                                     mode=\"train\",\n",
    "                                     device=device,\n",
    "                                     **inputs)\n",
    "        loss = gwe.generate_step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(epoch, step, loss.item(),gwe.last_predictions)\n",
    "        \n",
    "        step += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(len(train_dataloader))):\n",
    "    inputs = next(iter(train_dataloader))\n",
    "    gwe = GenerateWithEmbeddings(model,\n",
    "                                    tokenizer,\n",
    "                                    None, \n",
    "                                    None,\n",
    "                                    mode=\"train\",\n",
    "                                    device=device,\n",
    "                                    **inputs)\n",
    "\n",
    "    loss = gwe.generate_step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwe.generate()\n",
    "\n",
    "res = gwe.get_current_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''\n",
      "def go_to(location : str)\n",
      "def find(object : str)\n",
      "def pick_up(object : str)\n",
      "def put_down(object : str)\n",
      "def find(object : str)\n",
      "def ask(person : str, question : str, options: Optional[List[str]])\n",
      "def say(message : str)\n",
      "'''\n",
      "def main():\n",
      "    # Using the functions defined above, write a script to do the following: Go to the living room to get the TV Remote. Then go to the kitchen to pick up an apple. Give the TV Remote to Joydeep and the apple to Yash. They are both in their respective offices.\n",
      "\t<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = tokenizer.decode(train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/saxenaya/miniconda3/envs/robot_commands/lib/python3.10/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_hello_world():\n",
      "    print(\"Hello World!\")\n",
      "\n",
      "\n",
      "def print_hello_\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/santacoder\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/saxenaya/miniconda3/envs/robot_commands/lib/python3.10/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 145, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''\n",
      "def go_to(location : str)\n",
      "def find(object : str)\n",
      "def pick_up(object : str)\n",
      "def put_down(object : str)\n",
      "def find(object : str)\n",
      "def ask(person : str, question : str, options: Optional[List[str]])\n",
      "def say(message : str)\n",
      "'''\n",
      "def main():\n",
      "    # Using the functions defined above, write a script to do the following: Go to the living room to get the TV Remote. Then go to the kitchen to pick up an apple. Give the TV Remote to Joydeep and the apple to Yash. They are both in their respective offices.\n",
      "\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(example_input, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/saxenaya/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-88fa5373c749e3eb/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "access_token = \"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\"\n",
    "lua_data = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/lua\",split=\"train\", use_auth_token=access_token)\n",
    "lua_data.shuffle()\n",
    "lua_data = lua_data.train_test_split(test_size=0.1)\n",
    "train_data = lua_data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iterator = iter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_item = next(dataset_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local lfs = require(\"lib.lfs_ffi\")\\nlocal nfd = require(\"nfd\")\\nlocal physfs = require(\"lib.physfs\")\\nlocal requireUtils = require(\"utils.require\")\\nlocal threadHandler = require(\"utils.threads\")\\n\\nlocal hasRequest, request = requireUtils.tryrequire(\"lib.luajit-request.luajit-request\")\\n\\nlocal filesystem = {}\\n\\nfilesystem.supportWindowsInThreads = love.system.getOS() ~= \"OS X\"\\n\\nfunction filesystem.filename(path, sep)\\n    sep = sep or physfs.getDirSeparator()\\n\\n    return path:match(\"[^\" .. sep .. \"]+$\")\\nend\\n\\nfunction filesystem.dirname(path, sep)\\n    sep = sep or physfs.getDirSeparator()\\n\\n    return path:match(\"(.*\" .. sep .. \")\")\\nend\\n\\nfunction filesystem.joinpath(...)\\n    local paths = {...}\\n    local sep = physfs.getDirSeparator()\\n\\n    return table.concat(paths, sep):gsub(sep .. sep, sep)\\nend\\n\\nfunction filesystem.splitpath(s)\\n    local sep = physfs.getDirSeparator()\\n\\n    return string.split(s, sep)()\\nend\\n\\nfunction filesystem.samePath(path1, path2)\\n    local userOS = love.system.getOS()\\n\\n    if userOS == \"Windows\" then\\n        return path1:lower() == path2:lower()\\n\\n    else\\n        return path1 == path2\\n    end\\nend\\n\\n-- Maunal iteration for performance\\n-- String matching is expensive\\nfunction filesystem.fileExtension(path)\\n    for i = #path, 1, -1 do\\n        if path:byte(i, i) == 46 then\\n            return path:sub(i + 1, #path)\\n        end\\n    end\\n\\n    return path\\nend\\n\\n-- Maunal iteration for performance\\n-- String matching or getting ext just to sub is expensive\\nfunction filesystem.stripExtension(path)\\n    for i = #path, 1, -1 do\\n        if path:byte(i, i) == 46 then\\n            return path:sub(1, i - 1)\\n        end\\n    end\\n\\n    return path\\nend\\n\\nfunction filesystem.mkdir(path, mode)\\n    return lfs.mkdir(path, mode or 493) -- octal mode 755\\nend\\n\\nfilesystem.chdir = lfs.chdir\\nfilesystem.dir = lfs.dir\\nfilesystem.rmdir = lfs.rmdir\\n\\nfilesystem.remove = os.remove\\n\\n-- Use Unix paths\\nlocal function findRecursive(filenames, path, recursive, predicate, useYields, counter)\\n    counter = counter or 0\\n\\n    for _, filename in ipairs(love.filesystem.getDirectoryItems(path)) do\\n        local fullPath = path .. \"/\" .. filename\\n\\n        local fileInfo = love.filesystem.getInfo(fullPath)\\n\\n        if useYields and counter % 100 == 0 then\\n            coroutine.yield()\\n        end\\n\\n        if fileInfo then\\n            if fileInfo.type == \"file\" then\\n                if predicate and predicate(filename) then\\n                    table.insert(filenames, fullPath)\\n\\n                else\\n                    table.insert(filenames, fullPath)\\n                end\\n\\n                counter += 1\\n\\n            else\\n                if recursive then\\n                    findRecursive(filenames, fullPath, recursive, predicate, useYields, counter)\\n                end\\n            end\\n        end\\n    end\\nend\\n\\nfunction filesystem.getFilenames(path, recursive, filenames, predicate, useYields)\\n    useYields = useYields ~= false\\n    recursive = recursive ~= false\\n    filenames = filenames or {}\\n\\n    findRecursive(filenames, path, recursive, predicate, useYields)\\n\\n    return filenames\\nend\\n\\nfunction filesystem.pathAttributes(path)\\n    return lfs.attributes(path)\\nend\\n\\nfunction filesystem.listDir(path)\\n    return lfs.dir(path)\\nend\\n\\nfunction filesystem.isFile(path)\\n    local attrs = lfs.attributes(path)\\n\\n    return attrs and attrs.mode == \"file\"\\nend\\n\\nfunction filesystem.isDirectory(path)\\n    local attrs = lfs.attributes(path)\\n\\n    return attrs and attrs.mode == \"directory\"\\nend\\n\\nfunction filesystem.mtime(path)\\n    local attrs = lfs.attributes(path)\\n\\n    return attrs and attrs.modification or -1\\nend\\n\\n-- TODO - Test\\nfunction filesystem.copy(from, to)\\n    local fromFh = io.open(from, \"rb\")\\n\\n    if not fromFh then\\n        return false, \"Target file not found\"\\n    end\\n\\n    local toFh = io.open(to, \"wb\")\\n\\n    if not toFh then\\n        return false, \"Couldn\\'t create destination file\"\\n    end\\n\\n    toFh:write(fromFh:read(\"*a\"))\\n    toFh:close()\\n    fromFh:close()\\n\\n    return true\\nend\\n\\n-- Return thread if called with callback\\n-- Otherwise block and return the selected file\\nfunction filesystem.saveDialog(path, filter, callback)\\n    -- TODO - Verify arguments, documentation was very existant\\n\\n    if callback then\\n        if filesystem.supportWindowsInThreads then\\n            local code = [[\\n                local args = {...}\\n                local channelName, path, filter = unpack(args)\\n                local channel = love.thread.getChannel(channelName)\\n\\n                local nfd = require(\"nfd\")\\n\\n                local res = nfd.save(filter, nil, path)\\n                channel:push(res)\\n            ]]\\n\\n            return threadHandler.createStartWithCallback(code, callback, path, filter)\\n\\n        else\\n            callback(nfd.save(filter, nil, path))\\n\\n            return false, false\\n        end\\n\\n    else\\n        return nfd.save(filter, nil, path)\\n    end\\nend\\n\\n-- Return thread if called with callback\\n-- Otherwise block and return the selected file\\nfunction filesystem.openDialog(path, filter, callback)\\n    -- TODO - Verify arguments, documentation was very existant\\n\\n    if callback then\\n        if filesystem.supportWindowsInThreads then\\n            local code = [[\\n                local args = {...}\\n                local channelName, path, filter = unpack(args)\\n                local channel = love.thread.getChannel(channelName)\\n\\n                local nfd = require(\"nfd\")\\n\\n                local res = nfd.open(filter, nil, path)\\n                channel:push(res)\\n            ]]\\n\\n            return threadHandler.createStartWithCallback(code, callback, path, filter)\\n\\n        else\\n            callback(nfd.open(filter, nil, path))\\n\\n            return false, false\\n        end\\n\\n    else\\n        return nfd.open(filter, nil, path)\\n    end\\nend\\n\\nfunction filesystem.downloadURL(url, filename, headers)\\n    if not hasRequest then\\n        return false, nil\\n    end\\n\\n    local response = request.send(url, {\\n        headers = headers or {\\n            [\"User-Agent\"] = \"curl/7.78.0\",\\n            [\"Accept\"] = \"*/*\"\\n        }\\n    })\\n\\n    if response then\\n        local body, code = response.body, response.code\\n\\n        if body and code == 200 then\\n            filesystem.mkdir(filesystem.dirname(filename))\\n            local fh = io.open(filename, \"wb\")\\n\\n            if fh then\\n                fh:write(body)\\n                fh:close()\\n\\n                return true\\n            end\\n\\n        elseif code >= 300 and code <= 399 then\\n            local responseHeaders = response.headers\\n            local redirect = (responseHeaders[\"location\"] or responseHeaders[\"Location\"]):match(\"^%s*(.*)%s*$\")\\n            local newHeaders = table.shallowcopy(headers)\\n\\n            newHeaders[\"Referer\"] = url\\n\\n            return filesystem.downloadURL(redirect, filename, newHeaders)\\n        end\\n    end\\n\\n    return false\\nend\\n\\nfunction filesystem.copyFromLoveFilesystem(mountPoint, output, folder)\\n    local filesTablePath = folder and filesystem.joinpath(mountPoint, folder) or mountPoint\\n    local filesTable = love.filesystem.getDirectoryItems(filesTablePath)\\n\\n    local outputTarget = folder and filesystem.joinpath(output, folder) or output\\n    filesystem.mkdir(outputTarget)\\n\\n    for i, file <- filesTable do\\n        local path = folder and filesystem.joinpath(folder, file) or file\\n        local mountPath = filesystem.joinpath(mountPoint, path)\\n        local info = love.filesystem.getInfo(mountPath)\\n\\n        if info.type == \"file\" then\\n            local fh = io.open(filesystem.joinpath(output, path), \"wb\")\\n\\n            if fh then\\n                local data = love.filesystem.read(mountPath)\\n\\n                fh:write(data)\\n                fh:close()\\n            end\\n\\n        elseif info.type == \"directory\" then\\n            filesystem.copyFromLoveFilesystem(mountPoint, output, path)\\n        end\\n    end\\nend\\n\\n-- Unzip using phyfs unsandboxed mount system, and then manually copying out files\\nfunction filesystem.unzip(zipPath, outputDir)\\n    love.filesystem.mountUnsandboxed(zipPath, \"temp/\", 0)\\n\\n    filesystem.copyFromLoveFilesystem(\"temp\", outputDir)\\n\\n    love.filesystem.unmount(\"temp\")\\nend\\n\\nreturn filesystem'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_item['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_train_iterator = iter(train_dataset)\n",
    "next_train_item = next(example_train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2,  3013,   205,  1091,   227,  1012,    69,   306,    14,  2539,\n",
       "           227,    32,   227,   410,    15,   205,  1091,   227,  2479,    14,\n",
       "           955,   227,    32,   227,   410,    15,   205,  1091,   227,  2875,\n",
       "            69,   400,    14,   955,   227,    32,   227,   410,    15,   205,\n",
       "          1091,   227,   568,    69,  1413,    14,   955,   227,    32,   227,\n",
       "           410,    15,   205,  1091,   227,  2479,    14,   955,   227,    32,\n",
       "           227,   410,    15,   205,  1091,   227,   697,    14,  7665,   227,\n",
       "            32,   227,   410,    18,   227,  4740,   227,    32,   227,   410,\n",
       "            18,   227,  9260,    32,   227,  2882,    65,  1241,    65,   410,\n",
       "         12988,   205,  1091,   227, 14658,    14,  1180,   227,    32,   227,\n",
       "           410,    15,   205,  3013,   205,  1091,   227,  1203,  1442,   205,\n",
       "           502, 16678,   227,    59,  1822,   227,   733,   227, 15772,   227,\n",
       "         16896,   227, 10825,    18,   227,  1164,   227,    71,   227,  1874,\n",
       "           227,   306,   227,   401,   227,   733,   227, 25668,    32,   227,\n",
       "          4852,   227,   306,   227,   733,   227,   310, 37173,   227, 12487,\n",
       "           227,   306,   227,   453,   227,   733,   227, 18265,   227, 24803,\n",
       "            20,   227, 18571,   227,  1012,   227,   306,   227,   733,   227,\n",
       "            81,  3178,   279,   227,   306,   227,  2875,   227,   400,   227,\n",
       "           285,   227, 23104,    20, 22633,  2672,   227,   733,   227, 18265,\n",
       "           227, 24803,   227,   306,   227,  5514,    95,  5843,   227,   538,\n",
       "           227,   733,   227, 23104,   227,   306,   227,    63,  2004,    20,\n",
       "           227,  2814,    95,   227,  1457,   227,  8257,   227,   266,   227,\n",
       "         38734,   227, 11187,  3910,   227,    85,  1520,   614,    20,   205,\n",
       "           204]),\n",
       " 'labels': tensor([    2,  1012,    69,   306,   448,  2522, 37173,   227, 12487,   522,\n",
       "           205,  2479,   448, 18265,   227, 24803,   522,   205,  2875,    69,\n",
       "           400,   448, 18265,   227, 24803,   522,   205,  1012,    69,   306,\n",
       "           448,    49,  3178,   279,   522,   205,  2479,   448, 23104,   522,\n",
       "           205,  2875,    69,   400,   448, 23104,   522,   205,  1012,    69,\n",
       "           306,   448,  5514,    95,  5843, 16457,   227, 47063,   522,   205,\n",
       "           568,    69,  1413,   448, 18265,   227, 24803,   522,   205,  1012,\n",
       "            69,   306,   448,    63,  2004, 16457,   227, 47063,   522,   205,\n",
       "           568,    69,  1413,   448, 23104,   522])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_train_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot_commands",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
