{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saxenaya/miniconda3/envs/robot_commands/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.trainer_pt_utils import LengthGroupedSampler\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "\n",
    "from generate_with_embeddings import GenerateWithEmbeddings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"dryrun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantLengthDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n",
    "        Args:\n",
    "            tokenizer (Tokenizer): The processor used for proccessing the data.\n",
    "            dataset (dataset.Dataset): Dataset with text files.\n",
    "            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n",
    "            seq_length (int): Length of token sequences to return.\n",
    "            num_of_sequences (int): Number of token sequences to keep in buffer.\n",
    "            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        dataset,\n",
    "        infinite=False,\n",
    "        seq_length=1024,\n",
    "        num_of_sequences=1024,\n",
    "        chars_per_token=3.6,\n",
    "        content_field=\"content\",\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = (\n",
    "            tokenizer.eos_token_id if tokenizer.eos_token_id else \"<|endoftext|>\"\n",
    "        )\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.infinite = infinite\n",
    "        self.current_size = 0\n",
    "        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n",
    "        self.content_field = content_field\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.max_buffer_size:\n",
    "                    break\n",
    "                try:\n",
    "                    buffer.append(next(iterator)[self.content_field])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    if self.infinite:\n",
    "                        iterator = iter(self.dataset)\n",
    "                    else:\n",
    "                        more_examples = False\n",
    "                        break\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n",
    "            all_token_ids = []\n",
    "            for tokenized_input in tokenized_inputs:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    self.current_size += 1\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.LongTensor(input_ids),\n",
    "                        \"labels\": torch.LongTensor(input_ids),\n",
    "                    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"lua-training\", \n",
    "           config={\n",
    "                \"batch_size\": 32,\n",
    "                \"embedding_size\": 64,\n",
    "                \"lm_prefix_size\": 2048,\n",
    "                \"num_epochs\": 30,\n",
    "                \"learning_rate\": 3e-5,\n",
    "                \"checkpoint\": \"bigcode/santacoder\",\n",
    "                \"revision\": \"dedup-alt\",\n",
    "                \"device\": \"cuda:5\",\n",
    "                \"time_tag\": time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                \"record_step_every\": 1000\n",
    "           })\n",
    "\n",
    "checkpoint = wandb.config[\"checkpoint\"]\n",
    "revision = wandb.config[\"revision\"]\n",
    "device = wandb.config[\"device\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/saxenaya/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-88fa5373c749e3eb/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "access_token = \"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\"\n",
    "lua_data = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/lua\",split=\"train\", use_auth_token=access_token)\n",
    "lua_data.shuffle()\n",
    "lua_data = lua_data.train_test_split(test_size=0.1)\n",
    "train_data = lua_data[\"train\"]\n",
    "test_data = lua_data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint,revision=revision)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, revision=revision, trust_remote_code=True).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cl = ConstantLengthDataset(tokenizer, train_data, infinite=True, seq_length=1024, num_of_sequences=1024, chars_per_token=3.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")\n",
    "dataloader = torch.utils.data.DataLoader(train_data_cl, \n",
    "                                         batch_size=wandb.config[\"batch_size\"],\n",
    "                                         collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3724 > 2048). Running this sequence through the model will result in indexing errors\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  299, 10804,  3764,  ...,   450,   185,   185],\n",
      "        [ 1357,   577,  1030,  ...,   515,   287, 35700],\n",
      "        [  349,   287, 11574,  ...,    87,     8,   720],\n",
      "        ...,\n",
      "        [   62,   859,    76,  ...,  1050,   256, 24284],\n",
      "        [   62,  6235,    62,  ...,  1615,  3127,  1189],\n",
      "        [ 6582, 29868,   404,  ...,  3091,   258,   363]]), 'labels': tensor([[  299, 10804,  3764,  ...,   450,   185,   185],\n",
      "        [ 1357,   577,  1030,  ...,   515,   287, 35700],\n",
      "        [  349,   287, 11574,  ...,    87,     8,   720],\n",
      "        ...,\n",
      "        [   62,   859,    76,  ...,  1050,   256, 24284],\n",
      "        [   62,  6235,    62,  ...,  1615,  3127,  1189],\n",
      "        [ 6582, 29868,   404,  ...,  3091,   258,   363]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "for item in dataloader:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_code = tokenizer(train_data[\"content\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels = tokenized_code[\"input_ids\"].clone()\n",
    "if tokenizer._pad_token is not None:\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "tokenized_code[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = wandb.config[\"num_epochs\"]\n",
    "step = 0\n",
    "train_table_cols = [\"epoch\", \"step\", \"loss\", \"label\", \"predictions\"]\n",
    "next_step_in_table = wandb.config[\"record_step_every\"]\n",
    "train_table_rows = []\n",
    "train_table = wandb.Table(columns=train_table_cols)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/30 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.Encoding' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(indices, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m\"\u001b[39m, position\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m     \u001b[39m# Hack(ytzi): This is a hack to make sure that the model is not too big: just use 50 characters\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     tokenized_code \u001b[39m=\u001b[39m tokenizer(tokenizer\u001b[39m.\u001b[39mbos_token \u001b[39m+\u001b[39m train_data[i][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m][:\u001b[39m50\u001b[39m] \u001b[39m+\u001b[39m tokenizer\u001b[39m.\u001b[39meos_token,\n\u001b[1;32m     13\u001b[0m                                return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m                                padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m                                truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m     inputs \u001b[39m=\u001b[39m data_collator(tokenized_code) \n\u001b[1;32m     18\u001b[0m     gwe \u001b[39m=\u001b[39m GenerateWithEmbeddings(model,\n\u001b[1;32m     19\u001b[0m                                  tokenizer,\n\u001b[1;32m     20\u001b[0m                                  \u001b[39mNone\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m                                  mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m                                  device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     28\u001b[0m     losses \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_call(features)\n\u001b[1;32m     46\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/transformers/data/data_collator.py:732\u001b[0m, in \u001b[0;36mDataCollatorForLanguageModeling.torch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    729\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpad(examples, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, pad_to_multiple_of\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_to_multiple_of)\n\u001b[1;32m    730\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     batch \u001b[39m=\u001b[39m {\n\u001b[0;32m--> 732\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: _torch_collate_batch(examples, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer, pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of)\n\u001b[1;32m    733\u001b[0m     }\n\u001b[1;32m    735\u001b[0m \u001b[39m# If special token mask has been preprocessed, pop it from the dict.\u001b[39;00m\n\u001b[1;32m    736\u001b[0m special_tokens_mask \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mspecial_tokens_mask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/robot_commands/lib/python3.10/site-packages/transformers/data/data_collator.py:410\u001b[0m, in \u001b[0;36m_torch_collate_batch\u001b[0;34m(examples, tokenizer, pad_to_multiple_of)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(examples[\u001b[39m0\u001b[39m], (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, np\u001b[39m.\u001b[39mndarray)):\n\u001b[1;32m    408\u001b[0m     examples \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mtensor(e, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m examples]\n\u001b[0;32m--> 410\u001b[0m length_of_first \u001b[39m=\u001b[39m examples[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m    412\u001b[0m \u001b[39m# Check if padding is necessary.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m are_tensors_same_length \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m length_of_first \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m examples)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tokenizers.Encoding' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epoch\", position=0):\n",
    "    running_loss = 0\n",
    "\n",
    "    stats = {\"correct\": 0, \"total\": 0}\n",
    "    # Select a random minibatch of examples\n",
    "    indices = random.sample(range(0, len(train_data)), wandb.config[\"batch_size\"])\n",
    "\n",
    "    for i in tqdm(indices, desc=\"Batch\", position=1, leave=False):\n",
    "        # Hack(ytzi): This is a hack to make sure that the model is not too big: just use 50 characters\n",
    "        tokenized_code = tokenizer(tokenizer.bos_token + train_data[i][\"content\"][:50] + tokenizer.eos_token,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   padding=True,\n",
    "                                   truncation=True).to(device)\n",
    "        inputs = data_collator(tokenized_code) \n",
    "\n",
    "        gwe = GenerateWithEmbeddings(model,\n",
    "                                     tokenizer,\n",
    "                                     None, \n",
    "                                     prompt=prompt,\n",
    "                                     labels=labels,\n",
    "                                     max_length=tokenized_code[\"input_ids\"].shape[1] + 1,\n",
    "                                     mode=\"train\",\n",
    "                                     device=device)\n",
    "        \n",
    "\n",
    "        losses = []\n",
    "        while(gwe.can_generate_more()):\n",
    "            optimizer.zero_grad()\n",
    "            while(gwe.can_generate_more()):\n",
    "                loss = gwe.generate_step()\n",
    "                losses.append(loss)\n",
    "                stats[\"total\"] += 1\n",
    "                if gwe.is_last_prediction_correct:\n",
    "                    stats[\"correct\"] += 1\n",
    "\n",
    "            total_loss = torch.stack(losses).sum()\n",
    "\n",
    "            if step > next_step_in_table:\n",
    "                next_step_in_table += wandb.config[\"record_step_every\"]\n",
    "                train_table_rows.append([epoch, step, total_loss.item(), tokenizer.decode(tokenized_code[\"input_ids\"][0]), gwe.last_predictions])\n",
    "                train_table = wandb.Table(data=train_table_rows, columns=train_table_cols)\n",
    "            wandb.log({\"loss\": total_loss.item(), \"Training\": train_table})\n",
    "            step += 1\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "    running_loss /= BATCH_SIZE\n",
    "    wandb.log({\"running_loss\": running_loss, \"learning_rate\": lr_scheduler.get_lr()[0], \"accuracy_per_epoch\": stats[\"correct\"] / stats[\"total\"]})\n",
    "    step += 1\n",
    "    lr_scheduler.step()\n",
    "\n",
    "time_tag = wandb.config[\"time_tag\"]\n",
    "model.save_pretrained(f\"models/lua/{time_tag}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98282d30cb18f4fef664ba3265a90fdf66e62fd05a85bd94a248920656f0efc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
